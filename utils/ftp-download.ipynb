{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ftplib import FTP\n",
    "import utils\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "\n",
    "server='lhcftp.nlm.nih.gov'\n",
    "dirs_from_to=(1, 110)\n",
    "source_dirs=['Open-Access-Datasets/Pills/PillProjectDisc' + str(x) +'/images' for x in range(dirs_from_to[0], dirs_from_to[1]+1)]\n",
    "\n",
    "dest_base_dir=Path('Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FTP(server) as ftp:\n",
    "    print('Login into {}'.format(server))\n",
    "    ftp.login()\n",
    "\n",
    "    base_dir = ftp.pwd()\n",
    "    \n",
    "    for idx, source_dir in enumerate(source_dirs):\n",
    "        ftp.cwd(base_dir)\n",
    "        ftp.cwd(source_dir)\n",
    "        \n",
    "        dest_dir = dest_base_dir / str(idx)\n",
    "        \n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print('Current woring directory:', ftp.pwd())\n",
    "        print('Downloading tmp xml')\n",
    "        filename = 'images.xml'\n",
    "        dest_file = dest_dir / filename\n",
    "        with open(dest_file, 'wb') as f:\n",
    "            ftp.retrbinary('RETR ' + filename, f.write)\n",
    "\n",
    "        tree = ET.parse(dest_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        se = list(root)[0]\n",
    "        \n",
    "        print('Filtering xml (' + str(len(list(se))) + ' images)')\n",
    "        images = []\n",
    "        for e in list(se):\n",
    "            layout = e.find('Layout')\n",
    "            shadow = e.find('RatingShadow')\n",
    "            colors = e.findall('Color')\n",
    "            if (layout is not None and layout.text == \"MC_C3PI_REFERENCE_SEG_V1.6\") or \\\n",
    "               (shadow is not None and shadow.text == 'Soft') and \\\n",
    "               (colors is not None and len(colors) == 1):\n",
    "                images.append(e.find('File').find('Name').text)\n",
    "            else:\n",
    "                se.remove(e)\n",
    "        \n",
    "        print('saving xml in:', dest_file)\n",
    "        tree.write(dest_file)\n",
    "        \n",
    "        print(\"final images:\", len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in this part I check if there are more images with the same type of drug \n",
    "dirs = [x for x in dest_base_dir.iterdir() if x.is_dir()]\n",
    "\n",
    "ids = dict()\n",
    "\n",
    "expected_size = 0\n",
    "\n",
    "for d in dirs:\n",
    "    try:\n",
    "        tree = ET.parse(d / 'images.xml')\n",
    "    except ET.ParseError:\n",
    "        print('Parse error on {}'.format(d/'images.xml'))\n",
    "        continue\n",
    "    se = list(tree.getroot())[0]\n",
    "    \n",
    "    for e in list(se):\n",
    "        expected_size += int(e.find('File').find('Size').text)\n",
    "        \n",
    "        # i = e.find('ProprietaryName').text.lower()\n",
    "        # i = e.find('NDC11').text[5:9]\n",
    "        i = e.find('NDC9').text\n",
    "        if i not in ids:\n",
    "            ids[i] = []\n",
    "        ids[i].append(e.find('File').find('Name').text)\n",
    "\n",
    "sizes = dict()\n",
    "\n",
    "for k, e in ids.items():\n",
    "    if len(e) not in sizes:\n",
    "        sizes[len(e)] = []\n",
    "    sizes[len(e)].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  11 ids with   16 images\n",
      "  15 ids with    8 images\n",
      "Total ids: 26\n",
      "Total images: 296\n",
      "1.8 G will be needed to download all the images\n"
     ]
    }
   ],
   "source": [
    "sorted_sizes = list(sizes.keys())\n",
    "sorted_sizes.sort(reverse=True)\n",
    "\n",
    "total_ids = 0\n",
    "total_images = 0\n",
    "for k in sorted_sizes:\n",
    "    print('{:4} ids with {:4} images'.format(len(sizes[k]), k))\n",
    "    total_ids += len(sizes[k])\n",
    "    total_images += len(sizes[k]) * k\n",
    "    \n",
    "print('Total ids: {}'.format(total_ids))\n",
    "print('Total images: {}'.format(total_images))\n",
    "\n",
    "print(utils.bytes2human(expected_size), 'will be needed to download all the images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login into lhcftp.nlm.nih.gov\n",
      "Dataset/0\n",
      "Downloading   7/7\n",
      "Dataset/1\n",
      "Downloading   5/5\n",
      "Dataset/2\n",
      "Downloading   7/7\n",
      "Dataset/3\n",
      "Downloading   7/7\n",
      "Dataset/4\n",
      "Downloading   9/9\n",
      "Dataset/5\n",
      "Downloading  13/13\n",
      "Dataset/6\n",
      "Downloading   5/5\n",
      "Dataset/7\n",
      "Downloading   3/3\n",
      "Dataset/8\n",
      "Downloading   7/7\n",
      "Dataset/9\n",
      "Downloading   7/7\n",
      "Dataset/10\n",
      "Downloading   4/4\n",
      "Dataset/11\n",
      "Downloading   7/7\n",
      "Dataset/12\n",
      "Downloading  16/16\n",
      "Dataset/13\n",
      "Downloading  10/10\n",
      "Dataset/14\n",
      "Downloading   6/6\n",
      "Dataset/15\n",
      "Downloading   7/7\n",
      "Dataset/16\n",
      "Downloading   8/8\n",
      "Dataset/17\n",
      "Downloading   6/6\n",
      "Dataset/18\n",
      "Downloading   8/8\n",
      "Dataset/19\n",
      "Downloading   5/5\n",
      "Dataset/20\n",
      "Downloading   8/8\n",
      "Dataset/21\n",
      "Downloading   6/6\n",
      "Dataset/22\n",
      "Downloading   6/6\n",
      "Dataset/23\n",
      "Downloading   6/6\n",
      "Dataset/24\n",
      "Downloading   8/8\n",
      "Dataset/25\n",
      "Downloading   6/6\n",
      "Dataset/26\n",
      "Downloading   7/7\n",
      "Dataset/27\n",
      "Downloading   6/6\n",
      "Dataset/28\n",
      "Downloading  10/10\n",
      "Dataset/29\n",
      "Downloading   3/3\n",
      "Dataset/30\n",
      "Downloading   2/2\n",
      "Dataset/31\n",
      "Downloading   2/2\n",
      "Dataset/32\n",
      "Downloading   9/9\n",
      "Dataset/33\n",
      "Downloading   9/9\n",
      "Dataset/34\n",
      "Downloading   8/8\n",
      "Dataset/35\n",
      "Downloading   6/6\n",
      "Dataset/36\n",
      "Downloading   6/6\n",
      "Dataset/37\n",
      "Downloading   5/5\n",
      "Dataset/38\n",
      "Downloading   8/8\n",
      "Dataset/39\n",
      "Downloading   5/5\n",
      "Dataset/40\n",
      "Downloading   6/6\n",
      "Dataset/41\n",
      "Downloading   5/5\n",
      "Dataset/42\n",
      "Downloading   7/7\n",
      "Dataset/43\n",
      "Downloading   4/4\n",
      "Dataset/44\n",
      "Downloading  12/12\n",
      "Dataset/45\n",
      "Downloading   8/8\n",
      "Dataset/46\n",
      "Downloading   6/6\n",
      "Dataset/47\n",
      "Downloading  11/11\n",
      "Dataset/48\n",
      "Downloading   5/5\n",
      "Dataset/49\n",
      "Downloading   7/7\n",
      "Dataset/50\n",
      "Downloading  11/11\n",
      "Dataset/51\n",
      "Downloading   6/6\n",
      "Dataset/52\n",
      "Downloading   8/8\n",
      "Dataset/53\n",
      "Downloading   7/7\n",
      "Dataset/54\n",
      "Downloading  14/14\n",
      "Dataset/55\n",
      "Downloading   4/4\n",
      "Dataset/56\n",
      "Downloading   7/7\n",
      "Dataset/57\n",
      "Downloading  13/13\n",
      "Dataset/58\n",
      "Downloading   2/2\n",
      "Dataset/59\n",
      "Downloading   4/4\n",
      "Dataset/60\n",
      "Downloading   5/5\n",
      "Dataset/61\n",
      "Downloading   7/7\n",
      "Dataset/62\n",
      "Downloading   5/5\n",
      "Dataset/63\n",
      "Downloading   5/5\n",
      "Dataset/64\n",
      "Downloading   6/6\n",
      "Dataset/65\n",
      "Downloading   5/5\n",
      "Dataset/66\n",
      "Downloading   4/4\n",
      "Dataset/67\n",
      "Downloading  15/15\n",
      "Dataset/68\n",
      "Downloading   6/6\n",
      "Dataset/69\n",
      "Downloading   8/8\n",
      "Dataset/70\n",
      "Downloading   7/7\n",
      "Dataset/71\n",
      "Downloading   6/6\n",
      "Dataset/72\n",
      "Downloading   7/7\n",
      "Dataset/73\n",
      "Downloading   7/7\n",
      "Dataset/74\n",
      "Downloading   7/7\n",
      "Dataset/75\n",
      "Downloading  10/10\n",
      "Dataset/76\n",
      "Downloading   7/7\n",
      "Dataset/77\n",
      "Downloading   1/1\n",
      "Dataset/78\n",
      "Downloading   7/7\n",
      "Dataset/79\n",
      "Downloading   7/7\n",
      "Dataset/80\n",
      "Downloading   5/5\n",
      "Dataset/81\n",
      "Downloading  11/11\n",
      "Dataset/82\n",
      "Downloading  11/11\n",
      "Dataset/83\n",
      "Downloading   9/9\n",
      "Dataset/84\n",
      "Downloading  10/10\n",
      "Dataset/85\n",
      "Downloading   4/4\n",
      "Dataset/86\n",
      "Downloading   7/7\n",
      "Dataset/87\n",
      "Downloading   4/4\n",
      "Dataset/88\n",
      "Downloading   6/6\n",
      "Dataset/89\n",
      "Downloading   7/7\n",
      "Dataset/90\n",
      "Downloading   5/5\n",
      "Dataset/91\n",
      "Downloading   9/9\n",
      "Dataset/92\n",
      "Downloading   5/5\n",
      "Dataset/93\n",
      "Downloading  12/12\n",
      "Dataset/94\n",
      "Downloading   2/2\n",
      "Dataset/95\n",
      "Downloading  10/10\n",
      "Dataset/96\n",
      "Downloading   6/6\n",
      "Dataset/97\n",
      "Downloading   6/6\n",
      "Dataset/98\n",
      "Downloading   9/9\n",
      "Dataset/99\n",
      "Downloading   5/5\n",
      "Dataset/100\n",
      "Downloading  11/11\n",
      "Dataset/101\n",
      "Downloading   6/6\n",
      "Dataset/102\n",
      "Downloading   9/9\n",
      "Dataset/103\n",
      "Downloading   9/9\n",
      "Dataset/104\n",
      "Downloading   6/6\n",
      "Dataset/105\n",
      "Downloading   5/5\n",
      "Dataset/106\n",
      "Downloading  10/10\n",
      "Dataset/107\n",
      "Downloading   8/8\n",
      "Dataset/108\n",
      "Downloading  14/14\n",
      "Dataset/109\n",
      "Downloading   7/7\n"
     ]
    }
   ],
   "source": [
    "ids_to_download = sizes[30]\n",
    "download_imgs = True\n",
    "\n",
    "with FTP(server) as ftp:\n",
    "    print('Login into {}'.format(server))\n",
    "    ftp.login()\n",
    "\n",
    "    base_dir = ftp.pwd()\n",
    "    \n",
    "    for idx, source_dir in enumerate(source_dirs):\n",
    "        ftp.cwd(base_dir)\n",
    "        ftp.cwd(source_dir)\n",
    "        \n",
    "        dest_dir = dest_base_dir / str(idx)\n",
    "        \n",
    "        print(dest_dir)\n",
    "        \n",
    "        try:\n",
    "            tree = ET.parse(dest_dir / 'images.xml')\n",
    "        except ET.ParseError:\n",
    "            print('Parse error on {}'.format(dest_dir / 'images.xml'))\n",
    "            continue\n",
    "        se = list(tree.getroot())[0]\n",
    "        \n",
    "        images = []\n",
    "        \n",
    "        for e in list(se):\n",
    "            ndc = e.find('NDC9').text\n",
    "            if ndc in ids_to_download:\n",
    "                images.append(e.find('File').find('Name').text)\n",
    "        \n",
    "        # downloading\n",
    "        if download_imgs:\n",
    "            for i, img in enumerate(images):\n",
    "                dest_file = dest_dir / img\n",
    "                if (dest_file).exists():\n",
    "                    print(img, 'already downloaded!')\n",
    "                    continue\n",
    "                with open(dest_file, 'wb') as f:\n",
    "                    print('\\rDownloading {:3}/{}'.format(i + 1, len(images)), end='')\n",
    "                    ftp.retrbinary('RETR ' + img, f.write)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging dir 110/110\n",
      "Saving xml\n"
     ]
    }
   ],
   "source": [
    "# Merge in one dir\n",
    "\n",
    "import shutil\n",
    "\n",
    "merge_dir = dest_base_dir / 'merge'\n",
    "\n",
    "merge_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "merge_xml = ET.Element('MedicosConsultants')\n",
    "ET.SubElement(merge_xml, 'ImageExport')\n",
    "\n",
    "for i, d in enumerate(dirs):\n",
    "    print('\\rMerging dir {:3}/{}'.format(i + 1, len(dirs)), end='')\n",
    "    imgs = [x.name for x in d.iterdir() if x.suffix != '.xml']\n",
    "    \n",
    "    tree = ET.parse(d / 'images.xml')\n",
    "    \n",
    "    se = list(tree.getroot())[0]\n",
    "    \n",
    "    for e in list(se):\n",
    "        name = e.find('File').find('Name').text\n",
    "        if name in imgs:\n",
    "            merge_xml[0].append(e)\n",
    "    \n",
    "    for img in imgs:\n",
    "        shutil.move(str(d / img), str(merge_dir))\n",
    "\n",
    "print('\\nSaving xml')\n",
    "ET.ElementTree(merge_xml).write(merge_dir / 'images.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "\n",
    "shas = list(map(lambda x : x.text, merge_xml.findall('.//Sha1')))\n",
    "\n",
    "def list_duplicates(seq):\n",
    "  seen = set()\n",
    "  seen_add = seen.add\n",
    "  # adds all elements it doesn't know yet to seen and all other to seen_twice\n",
    "  seen_twice = set( x for x in seq if x in seen or seen_add(x) )\n",
    "  # turn the set into a list (as requested)\n",
    "  return list( seen_twice )\n",
    "\n",
    "print(list_duplicates(shas))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "d =  Counter(shas)\n",
    "res = [k for k, v in d.items() if v > 1]\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
