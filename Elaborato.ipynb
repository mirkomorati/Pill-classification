{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.svm import SVC\n",
    "from skimage.io import imread\n",
    "from skimage import img_as_float\n",
    "from skimage.transform import resize\n",
    "# from concurrent.futures import ThreadPoolExecutor as PoolExecutor\n",
    "# from concurrent.futures import as_completed\n",
    "from joblib import Parallel, delayed\n",
    "from time import time\n",
    "\n",
    "from pillclassification.feature_extraction import feature_extraction\n",
    "from pillclassification.functions import crop_center\n",
    "\n",
    "images_dir = Path('dataset/merge')\n",
    "filenames = [x for x in images_dir.iterdir() if x.suffix != '.xml']\n",
    "\n",
    "samples_num = len(filenames)\n",
    "feature_number = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating labels \n",
    "try:\n",
    "    tree = ET.parse(images_dir / 'images.xml')\n",
    "except ET.ParseError:\n",
    "    print('Parse error on {}'.format(images_dir / 'images.xml'))\n",
    "    exit(-1)\n",
    "\n",
    "se = list(tree.getroot())[0]\n",
    "\n",
    "labels_set = set()\n",
    "for e in list(se):\n",
    "    labels_set.add(e.find('NDC9').text)\n",
    "\n",
    "labels = sorted(list(labels_set))\n",
    "class_num = len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(f):\n",
    "    # loading the image \n",
    "    try:\n",
    "        img = imread(f)\n",
    "    except ValueError as e:\n",
    "        return None\n",
    "    \n",
    "    if img.shape[-1] == 4:\n",
    "        img = img[:,:,:3]\n",
    "    \n",
    "    # cropping in the center\n",
    "    img = crop_center(img, crop_scale=0.65)\n",
    "\n",
    "    # rescaling with fixed width\n",
    "    width = 600\n",
    "    img = resize(img, (int(img.shape[0] * (width / img.shape[1])), width), anti_aliasing=True)\n",
    "\n",
    "    # the img must be in float format \n",
    "    img = img_as_float(img)\n",
    "\n",
    "    # feature extraction \n",
    "    try:\n",
    "        hu, rgb_val = feature_extraction(img)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "    label = -1\n",
    "    for e in list(se):\n",
    "        if e.find('File').find('Name').text == f.name:\n",
    "            label = labels.index(e.find('NDC9').text)\n",
    "            break\n",
    "    \n",
    "    return np.append(hu, rgb_val), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extracting features\n",
    "x_data = np.zeros((samples_num, feature_number))\n",
    "y_data = np.zeros(samples_num, dtype=np.int32)\n",
    "\n",
    "def test(f):\n",
    "    return f\n",
    "\n",
    "with Parallel(n_jobs=12) as parallel:\n",
    "    s = time()\n",
    "    for i, r in enumerate(parallel(delayed(extract_features)(f) for f in filenames)):\n",
    "        if r is not None:\n",
    "            print('Data from image', i)\n",
    "            x_data[i, :], y_data[i] = r\n",
    "        else:\n",
    "            print('No data from image', i)\n",
    "            x_data[i, :], y_data[i] = [-1,-1]\n",
    "    print(time() - s)\n",
    "\n",
    "# printing and saving the features as npy file \n",
    "np.save('x_data_saved', x_data)\n",
    "np.save('y_data_saved', y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading features if already extracted\n",
    "x_data = np.load('x_data_saved.npy')\n",
    "y_data = np.load('y_data_saved.npy')\n",
    "\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)\n",
    "\n",
    "print(y_data)\n",
    "class_num = np.max(y_data) + 2\n",
    "samples_num = x_data.shape[0]\n",
    "\n",
    "print(class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dividing dataset\n",
    "from math import ceil\n",
    "\n",
    "# division factor\n",
    "factor = 2/3\n",
    "\n",
    "# calculating the number of elements in the train set\n",
    "train_n = 0\n",
    "test_n = 1\n",
    "for i in range(class_num):\n",
    "    y_len = len(y_data[y_data == i])\n",
    "    train_n += ceil(factor * y_len)\n",
    "    test_n += y_len - ceil(factor * y_len)\n",
    "\n",
    "print(train_n + test_n, x_data.shape[0])\n",
    "\n",
    "x_train = np.zeros_like(x_data[:train_n, :])\n",
    "y_train = np.zeros_like(y_data[:train_n])\n",
    "\n",
    "x_test = np.zeros_like(x_data[train_n:, :])\n",
    "y_test = np.zeros_like(y_data[train_n:])\n",
    "\n",
    "\n",
    "train_idx, test_idx = 0, 0\n",
    "for i in range(class_num):\n",
    "    x = x_data[y_data == i, :]\n",
    "    y = y_data[y_data == i]\n",
    "\n",
    "    train_n = ceil(factor * len(y))\n",
    "    test_n = len(y) - train_n\n",
    "\n",
    "    x_train[train_idx:train_idx+train_n, :] = x[:train_n, :]\n",
    "    y_train[train_idx:train_idx+train_n] = y[:train_n]\n",
    "\n",
    "    x_test[test_idx:test_idx+test_n, :] = x[train_n:, :]\n",
    "    y_test[test_idx:test_idx+test_n] = y[train_n:]\n",
    "\n",
    "    train_idx += train_n\n",
    "    test_idx += test_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SVC training\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "OVO = OneVsOneClassifier(make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-6, max_iter=100000)))\n",
    "OVR = OneVsRestClassifier(make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-6, max_iter=100000)))\n",
    "knn = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=1))\n",
    "\n",
    "clcOVO = OVO.fit(x_train, y_train) # fitting\n",
    "clcOVR = OVR.fit(x_train, y_train)\n",
    "clc_knn = knn.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = extract_features(filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "s1 = clcOVO.score(x_test, y_test)\n",
    "s2 = clcOVR.score(x_test, y_test)\n",
    "s3 = clc_knn.score(x_test, y_test)\n",
    "print(s1*100, s2*100, s3*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}