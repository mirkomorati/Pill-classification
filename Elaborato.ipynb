{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elaborato Teorie e Tecniche del Riconoscimento\n",
    "\n",
    "Nel 2016 la [National Library of Medicine](https://www.nlm.nih.gov/) ha proposto \n",
    "la sfida [\"Pill Image Recognition\"](https://pir.nlm.nih.gov/challenge)\n",
    "\n",
    "L'obiettivo è quello di implementare un metodo automatico per la classificazione \n",
    "di pillole a partire da un'immagine in modo da prevenire possibili errori umani \n",
    "che possano causare rischi per la salute.\n",
    "\n",
    "Per le configurazioni necessarie per il funzionamento dello script è possibile \n",
    "modificare i dati nella cella seguente. \n",
    "N.B. È sempre necessario eseguire questa cella prima di qualunque altra per non \n",
    "incorrere in errori.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# dataset directory: it should contain the images and the xml file \n",
    "DATASET_DIR = Path('dataset/merge')\n",
    "\n",
    "# directory of backgrounds: it should contains the background samples \n",
    "# used to generate images \n",
    "BACKGROUNDS_DIR = Path('utils/backgrounds')\n",
    "\n",
    "# path to the folder that is supposed to contain the datasets\n",
    "# saved in npy format\n",
    "SAVED_DATA_DIR = Path('saved_data')\n",
    "\n",
    "# number of image that should be generated foreach already segmented \n",
    "# image in the dataset\n",
    "AUTO_GENERATED_NUM = 10\n",
    "\n",
    "# number of feature's vectors foreach batch\n",
    "FEATURE_PER_BATCH = 500\n",
    "\n",
    "# This is the number of feature that the script will use \n",
    "# N.B. It is NOT a configurable parameter. It is constant and \n",
    "# it must not be changed.\n",
    "FEATURES_NUMBER = 10 \n",
    "\n",
    "# maximun number of jobs to use for parallelization\n",
    "MAX_JOBS = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Il dataset fornito per la sfida non include delle informazioni sulle immagini,\n",
    "mentre il dataset disponibile sul server FTP della NLM include quasi 1TB di immagini\n",
    "consumer di pillole con relative informazioni in formato XML.\n",
    "\n",
    "Abbiamo scelto di creare il nostro dataset considerando un numero di classi \n",
    "limitato (codice NDC9) e filtrando le immagini sulla base del rating di alcuni \n",
    "parametri (shadow, background, lighting).\n",
    "\n",
    "Per arricchire il dataset abbiamo implementato delle tecniche di data \n",
    "augmentation: sfruttando delle immagini con pillole già segmentate abbiamo introdotto \n",
    "una variabilità sul background, presenza di ombre e posa della pillola.\n",
    "\n",
    "<img src=\"images/data_aug.png\"/>\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "<img src=\"images/seg.png\"/>\n",
    "\n",
    "### Segmentazione\n",
    "\n",
    "La segmentazione è basata sulla trasformazione di Watershed, ottenuti i superpixel viene calcolato un Region Adjacency Graph (RAG) basato sui colori, in seguito le regioni vengono progressivamente unite in base alla similarità del colore. \n",
    "\n",
    "### ROI\n",
    "\n",
    "Per capire qual è la nostra ROI utilizziamo delle euristiche sui parametri delle regioni ottenute (solidity, similarità ad un'ellisse, dimensione dell'area).\n",
    "\n",
    "### Estrazione delle feature\n",
    "\n",
    "Abbiamo deciso di utilizzare come feature i momenti di Hu per la shape e i valori RGB del colore dominante nella regione, per un totale di 10 feature. Per ottenere il colore dominante applichiamo un KMeans sulla ROI ottenendo al più 3 cluster, in questo modo uniamo eventuali regioni di colore che si sono formate durante il merge dei nodi del RAG e dovremmo avere una più netta separazione tra pillola e background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import xml.etree.ElementTree as ET\n",
    "from skimage.io import imread\n",
    "from skimage import img_as_float\n",
    "from skimage.transform import resize\n",
    "from joblib import Parallel, delayed\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pillclassification.feature_extraction import feature_extraction\n",
    "from pillclassification.functions import crop_center, generate_image\n",
    "from utils.utils import tqdm_joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Loading all images filenames\n",
    "images_dir = DATASET_DIR\n",
    "filenames = [x for x in images_dir.iterdir() if x.suffix != '.xml']\n",
    "bg_dir = BACKGROUNDS_DIR\n",
    "bgs = [x for x in bg_dir.iterdir()]\n",
    "\n",
    "# Vars\n",
    "samples_num = len(filenames)\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "batches = list(chunks(filenames, FEATURE_PER_BATCH))\n",
    "\n",
    "# Calculating labels \n",
    "try:\n",
    "    tree = ET.parse(images_dir / 'images.xml')\n",
    "except ET.ParseError:\n",
    "    print('Parse error on {}'.format(images_dir / 'images.xml'))\n",
    "    exit(-1)\n",
    "\n",
    "se = list(tree.getroot())[0]\n",
    "\n",
    "labels_set = set()\n",
    "segmented = 0\n",
    "\n",
    "for e in list(se):\n",
    "    labels_set.add(e.find('NDC9').text)\n",
    "    layout = e.find('Layout')\n",
    "    if layout is not None and layout.text == \"MC_C3PI_REFERENCE_SEG_V1.6\":\n",
    "        segmented += 1\n",
    "\n",
    "labels = sorted(list(labels_set))\n",
    "class_num = len(labels)\n",
    "\n",
    "# Number of samples that will be used (also generated images)\n",
    "final_samples = segmented * AUTO_GENERATED_NUM + len(filenames) - segmented\n",
    "\n",
    "def extract_features(f):\n",
    "    # loading the image \n",
    "    try:\n",
    "        img = imread(f)\n",
    "    except ValueError as e:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    features = np.ndarray((0,10))\n",
    "    labels_ = []\n",
    "    \n",
    "    # img is rgba\n",
    "    if img.shape[-1] == 4:\n",
    "        for _ in range(AUTO_GENERATED_NUM):\n",
    "            generated = generate_image(img, bgs[np.random.randint(0, len(bgs))])\n",
    "            try:\n",
    "                hu, rgb_val = feature_extraction(generated)\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            label = -1\n",
    "            for e in list(se):\n",
    "                if e.find('File').find('Name').text == f.name:\n",
    "                    label = labels.index(e.find('NDC9').text)\n",
    "                    break\n",
    "            features = np.append(features, [np.append(hu, rgb_val)], axis=0)\n",
    "            labels_ = np.append(labels_, label)\n",
    "        return features, labels_\n",
    "    \n",
    "    # cropping in the center\n",
    "    img = crop_center(img, crop_scale=0.6)\n",
    "\n",
    "    # rescaling with fixed width\n",
    "    width = 600\n",
    "    img = resize(img, (int(img.shape[0] * (width / img.shape[1])), width), anti_aliasing=True)\n",
    "\n",
    "    # the img must be in float format \n",
    "    img = img_as_float(img)\n",
    "    \n",
    "    # feature extraction\n",
    "    try:\n",
    "        hu, rgb_val = feature_extraction(img)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "    label = -1\n",
    "    for e in list(se):\n",
    "        if e.find('File').find('Name').text == f.name:\n",
    "            label = labels.index(e.find('NDC9').text)\n",
    "            break\n",
    "    \n",
    "    features = np.append(features, [np.append(hu, rgb_val)], axis=0)\n",
    "    labels_ = np.append(labels_, label)\n",
    "    return features, labels_\n",
    "\n",
    "# creating dirs if they do not exists\n",
    "(SAVED_DATA_DIR / 'x').mkdir(parents=True, exist_ok=True)\n",
    "(SAVED_DATA_DIR / 'y').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Extracting features\n",
    "with tqdm_joblib(tqdm(desc=\"Feature extraction\", total=len(filenames))) as progress_bar:\n",
    "    with Parallel(n_jobs=MAX_JOBS) as parallel:\n",
    "        for idx, batch in enumerate(batches):\n",
    "            x_data = np.ndarray((0, FEATURES_NUMBER))\n",
    "            y_data = np.array([], dtype=np.int32)\n",
    "            for _, r in enumerate(parallel(delayed(extract_features)(f) for f in batch)):\n",
    "                if r is not None:\n",
    "                    x_data = np.concatenate((x_data, r[0]))\n",
    "                    y_data = np.concatenate((y_data, r[1]))\n",
    "\n",
    "            # Saving the features as npy file\n",
    "            np.save(SAVED_DATA_DIR / 'x' / str(idx), x_data)\n",
    "            np.save(SAVED_DATA_DIR / 'y' / str(idx), y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import sys\n",
    "import numpy as np \n",
    "\n",
    "# Load all features\n",
    "\n",
    "x_data = np.ndarray((0, FEATURES_NUMBER))\n",
    "y_data = np.array([], dtype=np.int32)\n",
    "\n",
    "for x in (SAVED_DATA_DIR / 'x').iterdir():\n",
    "    if x.is_file() and x.suffix == '.npy':\n",
    "        y = SAVED_DATA_DIR / 'y' / x.name \n",
    "        \n",
    "        if not y.exists():\n",
    "            raise RuntimeError(\"Can't find y data for \" + x.name + ' data file')\n",
    "        \n",
    "    try:\n",
    "        x_data = np.concatenate((x_data, np.load(x)))\n",
    "        y_data = np.concatenate((y_data, np.load(y)))\n",
    "    except FileNotFoundError as e:\n",
    "        print('cannot load data for the batch {}.'.format(str(x.stem)) + \n",
    "              'Try to extract the features again', file=sys.stderr)\n",
    "        raise e\n",
    "\n",
    "# Clear data from nan\n",
    "idx = np.isnan(x_data[:])\n",
    "idx = np.where(np.any(idx == True, axis=1))\n",
    "\n",
    "for i in idx:\n",
    "    x_data = np.delete(x_data, i, 0)\n",
    "    y_data = np.delete(y_data, i, 0)\n",
    "    \n",
    "# Divide dataset in train + test\n",
    "\n",
    "idx_sort = np.argsort(y_data)\n",
    "\n",
    "y_sorted = y_data.take(idx_sort, 0)\n",
    "x_sorted = x_data.take(idx_sort, 0)\n",
    "\n",
    "j = 0\n",
    "\n",
    "idx_train = []\n",
    "\n",
    "cnt = Counter() \n",
    "\n",
    "for y in y_sorted:\n",
    "    cnt[int(y)] += 1\n",
    "\n",
    "for i in range(len(cnt)):\n",
    "    train_n = int(cnt[i] * 0.9)\n",
    "    for e in y_sorted[j:]:\n",
    "        if e == i:\n",
    "            break\n",
    "        j += 1\n",
    "    idx_train.extend(range(j, train_n + j))\n",
    "\n",
    "idx_test = np.delete(list(range(len(y_data))), idx_train, 0)\n",
    "X = x_sorted\n",
    "Y = y_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rilevamento delle anomalie\n",
    "\n",
    "Per 'pulire' il dataset utilizziamo l'algoritmo di Isolation Forest. L'algoritmo isola le osservazioni selezionando randomicamente una feature e un valore compreso tra il massimo e il minimo della feature selezionata. Dato che un partizionamento può essere rappresentanto da una struttura ad albero, il numero di partizionamenti richiesti per isolare un campione sono al massimo la lunghezza del percorso dalla radice al nodo di terminazione. Questa lunghezza di percorso, mediata su una foresta di alberi random, diventa la nostra funzione di decisione.\n",
    "\n",
    "Quando una foresta di alberi random produce un percorso più corto per un campione, allora quasi sicuramente è un'anomalia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Outlier detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso = IsolationForest(warm_start=True)\n",
    "yhat = iso.fit_predict(X[idx_train])\n",
    "\n",
    "# Select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train = X[idx_train]\n",
    "y_train = Y[idx_train]\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "\n",
    "\n",
    "yhat = iso.fit_predict(X[idx_test])\n",
    "\n",
    "mask = yhat != -1\n",
    "X_test = X[idx_test]\n",
    "y_test = Y[idx_test]\n",
    "X_test, y_test = X_test[mask, :], y_test[mask]\n",
    "\n",
    "# Summarize the shape of the updated training dataset\n",
    "print('Total: {}\\nTrain: {}\\nTest: {}'.format(X.shape[0], X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificazione delle feature\n",
    "\n",
    "Abbiamo provato a mettere a confronto quattro classificatori:\n",
    "\n",
    "- Regressione logistica:\n",
    "- SVM lineare:\n",
    "- SVM non lineare:\n",
    "- Gradient boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from utils.utils import tqdm_joblib\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def get_name(estimator):\n",
    "    name = estimator.__class__.__name__\n",
    "    if name == 'Pipeline':\n",
    "        name = [get_name(est[1]) for est in estimator.steps]\n",
    "        name = ' + '.join(name)\n",
    "    return name\n",
    "\n",
    "\n",
    "# list of (estimator, param_grid), where param_grid is used in GridSearchCV\n",
    "classifiers = [\n",
    "    (make_pipeline(\n",
    "        KBinsDiscretizer(encode='onehot'),\n",
    "        LogisticRegression(random_state=0)), {\n",
    "            'kbinsdiscretizer__n_bins': np.arange(2, 10),\n",
    "            'logisticregression__C': np.logspace(-2, 7, 10),\n",
    "        }),\n",
    "    (make_pipeline(\n",
    "        KBinsDiscretizer(encode='onehot'),\n",
    "        LinearSVC(random_state=0, tol=1e-6, max_iter=10000)), {\n",
    "            'kbinsdiscretizer__n_bins': np.arange(2, 10),\n",
    "            'linearsvc__C': np.logspace(-2, 7, 10),\n",
    "        }),\n",
    "    (GradientBoostingClassifier(n_estimators=100, random_state=0), {\n",
    "        'learning_rate': np.logspace(-4, 0, 10)\n",
    "    }),\n",
    "    (SVC(random_state=0, tol=1e-6, max_iter=10000), {\n",
    "        'C': np.logspace(-2, 7, 10)\n",
    "    }),\n",
    "]\n",
    "\n",
    "names = [get_name(e) for e, g in classifiers]\n",
    "clfs = []\n",
    "\n",
    "scores = dict()\n",
    "maes = dict()\n",
    "\n",
    "def evaluate_clf(estimator, param_grid, n_jobs):\n",
    "    clf = GridSearchCV(estimator=estimator, param_grid=param_grid, n_jobs=n_jobs)\n",
    "    with ignore_warnings(category=ConvergenceWarning):\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "    yhat = clf.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, yhat)\n",
    "    score = clf.score(X_test, y_test)\n",
    "\n",
    "    return get_name(estimator), score, mae\n",
    "\n",
    "\n",
    "\n",
    "with tqdm_joblib(tqdm(desc=\"Classifiers evaluation\", total=len(classifiers))) as progress_bar:\n",
    "    jobs = len(classifiers) if len(classifiers) < MAX_JOBS else MAX_JOBS\n",
    "\n",
    "    if MAX_JOBS - jobs >= len(classifiers):\n",
    "        jobs_for_grid = (MAX_JOBS - jobs) //  len(classifiers)\n",
    "    else:\n",
    "        jobs_for_grid = 1\n",
    "    with Parallel(n_jobs=MAX_JOBS) as parallel:\n",
    "        for name, score, mae in parallel(delayed(evaluate_clf)(estimator=e, param_grid=p, n_jobs=jobs_for_grid) for e, p in classifiers):\n",
    "            print('\\n{} MAE: {:.3f} Score: {:.3f}'.format(name, mae, score), end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}