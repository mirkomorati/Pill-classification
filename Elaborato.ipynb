{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elaborato Teorie e Tecniche del Riconoscimento \n",
    "\n",
    "Per l'Elaborato è stato scelto il compito di sviluppare un programma automatico \n",
    "che possa classificare un farmaco partendo dall'immagine di una pillola.\n",
    "\n",
    "Questa idea è ispirata dalla [challenge](https://pir.nlm.nih.gov/challenge) \n",
    "proposta dalla [National Library of Medicine](https://www.nlm.nih.gov/) americana.\n",
    "\n",
    "## Indice\n",
    "- todo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Il dataset\n",
    "\n",
    "Si è scelto di limitare il dataset proposto per la sfida selezionando solo le immagini \n",
    "cib caratteristiche adatte ed utilizzando delle tecniche di Data augmentation \n",
    "per aumentare il training set. \n",
    "\n",
    "Questa scelta è dettata dal compromesso dall'esigenza di ottenere un dataset di immagini \n",
    "\"Buone\" e facilmente segmentabili e quella di avere un numero sufficiente di immagini per \n",
    "il training dei classificatori.## Il dataset\n",
    "\n",
    "Si è scelto di limitare il dataset proposto per la sfida selezionando solo le immagini \n",
    "cib caratteristiche adatte ed utilizzando delle tecniche di Data augmentation \n",
    "per aumentare il training set. \n",
    "\n",
    "Questa scelta è dettata dal compromesso dall'esigenza di ottenere un dataset di immagini \n",
    "\"Buone\" e facilmente segmentabili e quella di avere un numero sufficiente di immagini per \n",
    "il training dei classificatori.\n",
    "\n",
    "### Feature extraction e Creazione del dataset \n",
    "\n",
    "Nella seguente sezione di codice iniziamo caricando il dataset di immagini \"buone\" selezionate \n",
    "a mano. Si suppone cohe queste immagini siano nel path `dataset/merge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import xml.etree.ElementTree as ET\n",
    "from skimage.io import imread\n",
    "from skimage import img_as_float\n",
    "from skimage.transform import resize\n",
    "from joblib import Parallel, delayed\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pillclassification.feature_extraction import feature_extraction\n",
    "from pillclassification.functions import crop_center, generate_image\n",
    "from utils.utils import tqdm_joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Loading all images filenames\n",
    "images_dir = Path('utils/Dataset/merge')\n",
    "filenames = [x for x in images_dir.iterdir() if x.suffix != '.xml']\n",
    "bg_dir = Path('utils/backgrounds')\n",
    "bgs = [x for x in bg_dir.iterdir()]\n",
    "\n",
    "# Vars\n",
    "samples_num = len(filenames)\n",
    "feature_number = 10\n",
    "\n",
    "generate_n = 10\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "per_batch = 500\n",
    "batches = list(chunks(filenames, per_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating labels \n",
    "try:\n",
    "    tree = ET.parse(images_dir / 'images.xml')\n",
    "except ET.ParseError:\n",
    "    print('Parse error on {}'.format(images_dir / 'images.xml'))\n",
    "    exit(-1)\n",
    "\n",
    "se = list(tree.getroot())[0]\n",
    "\n",
    "labels_set = set()\n",
    "segmented = 0\n",
    "\n",
    "for e in list(se):\n",
    "    labels_set.add(e.find('NDC9').text)\n",
    "    layout = e.find('Layout')\n",
    "    if layout is not None and layout.text == \"MC_C3PI_REFERENCE_SEG_V1.6\":\n",
    "        segmented += 1\n",
    "\n",
    "labels = sorted(list(labels_set))\n",
    "class_num = len(labels)\n",
    "\n",
    "final_samples = segmented * generate_n + len(filenames) - segmented\n",
    "\n",
    "# Number of samples that will be used (also generated images)\n",
    "print(final_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(f):\n",
    "    # loading the image \n",
    "    try:\n",
    "        img = imread(f)\n",
    "    except ValueError as e:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    features = np.ndarray((0,10))\n",
    "    labels_ = []\n",
    "    \n",
    "    # img is rgba\n",
    "    if img.shape[-1] == 4:\n",
    "        for _ in range(generate_n):\n",
    "            generated = generate_image(img, bgs[np.random.randint(0, len(bgs))])\n",
    "            try:\n",
    "                hu, rgb_val = feature_extraction(generated)\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            label = -1\n",
    "            for e in list(se):\n",
    "                if e.find('File').find('Name').text == f.name:\n",
    "                    label = labels.index(e.find('NDC9').text)\n",
    "                    break\n",
    "            features = np.append(features, [np.append(hu, rgb_val)], axis=0)\n",
    "            labels_ = np.append(labels_, label)\n",
    "        return features, labels_\n",
    "    \n",
    "    # cropping in the center\n",
    "    img = crop_center(img, crop_scale=0.6)\n",
    "\n",
    "    # rescaling with fixed width\n",
    "    width = 600\n",
    "    img = resize(img, (int(img.shape[0] * (width / img.shape[1])), width), anti_aliasing=True)\n",
    "\n",
    "    # the img must be in float format \n",
    "    img = img_as_float(img)\n",
    "    \n",
    "    # feature extraction\n",
    "    try:\n",
    "        hu, rgb_val = feature_extraction(img)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "    label = -1\n",
    "    for e in list(se):\n",
    "        if e.find('File').find('Name').text == f.name:\n",
    "            label = labels.index(e.find('NDC9').text)\n",
    "            break\n",
    "    \n",
    "    features = np.append(features, [np.append(hu, rgb_val)], axis=0)\n",
    "    labels_ = np.append(labels_, label)\n",
    "    return features, labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting features\n",
    "\n",
    "with tqdm_joblib(tqdm(desc=\"Feature extraction\", total=len(filenames))) as progress_bar:\n",
    "    with Parallel(n_jobs=12) as parallel:\n",
    "        for idx, batch in enumerate(batches):\n",
    "            x_data = np.ndarray((0, feature_number))\n",
    "            y_data = np.array([], dtype=np.int32)\n",
    "            for _, r in enumerate(parallel(delayed(extract_features)(f) for f in batch)):\n",
    "                if r is not None:\n",
    "                    x_data = np.concatenate((x_data, r[0]))\n",
    "                    y_data = np.concatenate((y_data, r[1]))\n",
    "\n",
    "            # Saving the features as npy file\n",
    "            np.save('x{}_data_saved'.format(idx), x_data)\n",
    "            np.save('y{}_data_saved'.format(idx), y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np \n",
    "\n",
    "# Load all features\n",
    "\n",
    "x_data = np.ndarray((0, feature_number))\n",
    "y_data = np.array([], dtype=np.int32)\n",
    "\n",
    "for i in range(len(batches)):\n",
    "    x_data = np.concatenate((x_data, np.load('x{}_data_saved.npy'.format(i))))\n",
    "    y_data = np.concatenate((y_data, np.load('y{}_data_saved.npy'.format(i))))\n",
    "    \n",
    "# Clear data from nan\n",
    "idx = np.isnan(x_data[:])\n",
    "idx = np.where(np.any(idx == True, axis=1))\n",
    "\n",
    "for i in idx:\n",
    "    x_data = np.delete(x_data, i, 0)\n",
    "    y_data = np.delete(y_data, i, 0)\n",
    "    \n",
    "# Divide dataset in train + test\n",
    "\n",
    "idx_sort = np.argsort(y_data)\n",
    "\n",
    "y_sorted = y_data.take(idx_sort, 0)\n",
    "x_sorted = x_data.take(idx_sort, 0)\n",
    "\n",
    "j = 0\n",
    "\n",
    "idx_train = []\n",
    "\n",
    "cnt = Counter() \n",
    "\n",
    "for y in y_sorted:\n",
    "    cnt[int(y)] += 1\n",
    "\n",
    "for i in range(class_num):\n",
    "    train_n = int(cnt[i] * 0.9)\n",
    "    for e in y_sorted[j:]:\n",
    "        if e == i:\n",
    "            break\n",
    "        j += 1\n",
    "    idx_train.extend(range(j, train_n + j))\n",
    "\n",
    "idx_test = np.delete(list(range(len(y_data))), idx_train, 0)\n",
    "X = x_sorted\n",
    "Y = y_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Outlier detection\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso = IsolationForest(warm_start=True)\n",
    "yhat = iso.fit_predict(X[idx_train])\n",
    "\n",
    "# Select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train = X[idx_train]\n",
    "y_train = Y[idx_train]\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "\n",
    "\n",
    "yhat = iso.fit_predict(X[idx_test])\n",
    "\n",
    "mask = yhat != -1\n",
    "X_test = X[idx_test]\n",
    "y_test = Y[idx_test]\n",
    "X_test, y_test = X_test[mask, :], y_test[mask]\n",
    "\n",
    "# Summarize the shape of the updated training dataset\n",
    "print('Total: {}\\nTrain: {}\\nTest: {}'.format(X.shape[0], X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "def get_name(estimator):\n",
    "    name = estimator.__class__.__name__\n",
    "    if name == 'Pipeline':\n",
    "        name = [get_name(est[1]) for est in estimator.steps]\n",
    "        name = ' + '.join(name)\n",
    "    return name\n",
    "\n",
    "\n",
    "# list of (estimator, param_grid), where param_grid is used in GridSearchCV\n",
    "classifiers = [\n",
    "    (make_pipeline(\n",
    "        KBinsDiscretizer(encode='onehot'),\n",
    "        LogisticRegression(random_state=0)), {\n",
    "            'kbinsdiscretizer__n_bins': np.arange(2, 10),\n",
    "            'logisticregression__C': np.logspace(-2, 7, 10),\n",
    "        }),\n",
    "    (make_pipeline(\n",
    "        KBinsDiscretizer(encode='onehot'),\n",
    "        LinearSVC(random_state=0, tol=1e-6, max_iter=10000)), {\n",
    "            'kbinsdiscretizer__n_bins': np.arange(2, 10),\n",
    "            'linearsvc__C': np.logspace(-2, 7, 10),\n",
    "        }),\n",
    "    (GradientBoostingClassifier(n_estimators=100, random_state=0), {\n",
    "        'learning_rate': np.logspace(-4, 0, 10)\n",
    "    }),\n",
    "    (SVC(random_state=0, tol=1e-6, max_iter=10000), {\n",
    "        'C': np.logspace(-2, 7, 10)\n",
    "    }),\n",
    "]\n",
    "\n",
    "names = [get_name(e) for e, g in classifiers]\n",
    "\n",
    "clfs = []\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "for est_idx, (name, (estimator, param_grid)) in enumerate(zip(names, classifiers)):\n",
    "    clf = GridSearchCV(estimator=estimator, param_grid=param_grid, n_jobs = -1)\n",
    "    clfs.append(clf)\n",
    "    with ignore_warnings(category=ConvergenceWarning):\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    yhat = clf.predict(X_test)\n",
    "    \n",
    "    # Evaluate predictions\n",
    "    mae = mean_absolute_error(y_test, yhat)\n",
    "    \n",
    "    score = clf.score(X_test, y_test)\n",
    "    print('{} MAE: {:.3f} Score: {:.3f}'.format(name, mae, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "print('Predicted: {}, Truth: {}'.format(clfs[4].predict([X[idx_test][i]])[0], Y[idx_test][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x[0])\n",
    "# print(y_data)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# r = lin_clf.predict([x[0]])\n",
    "# print(lin_clf.decision_function([x[0]]))\n",
    "\n",
    "print(lin_clf.score(X[idx_test], Y[idx_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Plotting decision regions\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "# Decision region for feature 3 = 1.5\n",
    "value = 2\n",
    "# Plot training sample with feature 3 = 1.5 +/- 0.75\n",
    "width = 0.75\n",
    "\n",
    "feature_values = {i:value for i in range(2, 10)}  \n",
    "feature_width = {i:value for i in range(2, 10)}  \n",
    "\n",
    "plot_decision_regions(X, Y.astype(np.integer), clf=lin_clf,\n",
    "                      filler_feature_values=feature_values,\n",
    "                      filler_feature_ranges=feature_width,\n",
    "                      res=0.02, ax=ax)\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('Feature 3 = {}'.format(value))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}